#!/usr/bin/env python3
"""
æœ¬åœ°æ¨¡å‹æ¥å£
æ”¯æŒå¤šç§æœ¬åœ°å¤§æ¨¡å‹éƒ¨ç½²æ–¹æ¡ˆï¼ŒåŒ…æ‹¬Ollamaã€LM Studioã€vLLMç­‰
"""

import requests
import json
import os
import time
from typing import Optional, Dict, Any

class LocalModelInterface:
    """æœ¬åœ°å¤§æ¨¡å‹æ¥å£ç±»"""
    
    def __init__(self, model_type: str = "ollama", **kwargs):
        """
        åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹æ¥å£
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ ("ollama", "lmstudio", "vllm", "openai_local")
            **kwargs: å…¶ä»–é…ç½®å‚æ•°
        """
        self.model_type = model_type.lower()
        self.config = self._get_default_config()
        self.config.update(kwargs)
        
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "ollama": {
                "base_url": "http://localhost:11434",
                "model_name": "qwen2.5:7b",
                "timeout": 60
            },
            "lmstudio": {
                "base_url": "http://localhost:1234",
                "model_name": "local-model",
                "timeout": 60
            },
            "vllm": {
                "base_url": "http://localhost:8000",
                "model_name": "qwen2.5-7b",
                "timeout": 60
            },
            "openai_local": {
                "base_url": "http://localhost:8080",
                "model_name": "local-model",
                "timeout": 60
            }
        }
    
    def chat_completion(self, messages: list, **kwargs) -> Optional[str]:
        """
        è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹è¿›è¡Œå¯¹è¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{"role": "user", "content": "..."}]
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            if self.model_type == "ollama":
                return self._call_ollama(messages, **kwargs)
            elif self.model_type == "lmstudio":
                return self._call_lmstudio(messages, **kwargs)
            elif self.model_type == "vllm":
                return self._call_vllm(messages, **kwargs)
            elif self.model_type == "openai_local":
                return self._call_openai_local(messages, **kwargs)
            else:
                print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
                return None
        except Exception as e:
            print(f"âŒ æœ¬åœ°æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _call_ollama(self, messages: list, **kwargs) -> Optional[str]:
        """è°ƒç”¨Ollamaæ¨¡å‹"""
        config = self.config["ollama"]
        
        # æ„å»ºOllamaæ ¼å¼çš„è¯·æ±‚
        prompt = self._messages_to_prompt(messages)
        
        payload = {
            "model": config["model_name"],
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": kwargs.get("temperature", 0.3),
                "top_p": kwargs.get("top_p", 0.9),
                "max_tokens": kwargs.get("max_tokens", 2000)
            }
        }
        
        try:
            response = requests.post(
                f"{config['base_url']}/api/generate",
                json=payload,
                timeout=config["timeout"]
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "")
            else:
                print(f"âŒ Ollama APIé”™è¯¯: {response.status_code} - {response.text}")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ Ollamaè¿æ¥å¤±è´¥: {e}")
            return None
    
    def _call_lmstudio(self, messages: list, **kwargs) -> Optional[str]:
        """è°ƒç”¨LM Studioæ¨¡å‹"""
        config = self.config["lmstudio"]
        
        payload = {
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.3),
            "max_tokens": kwargs.get("max_tokens", 2000),
            "stream": False
        }
        
        try:
            response = requests.post(
                f"{config['base_url']}/v1/chat/completions",
                json=payload,
                timeout=config["timeout"]
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"âŒ LM Studio APIé”™è¯¯: {response.status_code} - {response.text}")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ LM Studioè¿æ¥å¤±è´¥: {e}")
            return None
    
    def _call_vllm(self, messages: list, **kwargs) -> Optional[str]:
        """è°ƒç”¨vLLMæ¨¡å‹"""
        config = self.config["vllm"]
        
        payload = {
            "model": config["model_name"],
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.3),
            "max_tokens": kwargs.get("max_tokens", 2000),
            "stream": False
        }
        
        try:
            response = requests.post(
                f"{config['base_url']}/v1/chat/completions",
                json=payload,
                timeout=config["timeout"]
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"âŒ vLLM APIé”™è¯¯: {response.status_code} - {response.text}")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ vLLMè¿æ¥å¤±è´¥: {e}")
            return None
    
    def _call_openai_local(self, messages: list, **kwargs) -> Optional[str]:
        """è°ƒç”¨æœ¬åœ°OpenAIå…¼å®¹API"""
        config = self.config["openai_local"]
        
        payload = {
            "model": config["model_name"],
            "messages": messages,
            "temperature": kwargs.get("temperature", 0.3),
            "max_tokens": kwargs.get("max_tokens", 2000),
            "stream": False
        }
        
        try:
            response = requests.post(
                f"{config['base_url']}/v1/chat/completions",
                json=payload,
                timeout=config["timeout"]
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["choices"][0]["message"]["content"]
            else:
                print(f"âŒ æœ¬åœ°OpenAI APIé”™è¯¯: {response.status_code} - {response.text}")
                return None
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ æœ¬åœ°OpenAIè¿æ¥å¤±è´¥: {e}")
            return None
    
    def _messages_to_prompt(self, messages: list) -> str:
        """å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºæç¤ºè¯"""
        prompt = ""
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            
            if role == "system":
                prompt += f"ç³»ç»Ÿ: {content}\n\n"
            elif role == "user":
                prompt += f"ç”¨æˆ·: {content}\n\n"
            elif role == "assistant":
                prompt += f"åŠ©æ‰‹: {content}\n\n"
        
        prompt += "åŠ©æ‰‹: "
        return prompt
    
    def test_connection(self) -> bool:
        """æµ‹è¯•æœ¬åœ°æ¨¡å‹è¿æ¥"""
        test_messages = [{"role": "user", "content": "ä½ å¥½ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'"}]
        
        print(f"ğŸ§ª æµ‹è¯• {self.model_type} è¿æ¥...")
        response = self.chat_completion(test_messages)
        
        if response:
            print(f"âœ… {self.model_type} è¿æ¥æˆåŠŸ")
            print(f"ğŸ“ æµ‹è¯•å“åº”: {response[:50]}...")
            return True
        else:
            print(f"âŒ {self.model_type} è¿æ¥å¤±è´¥")
            return False

def create_local_model_interface(model_type: str = "ollama", **kwargs) -> LocalModelInterface:
    """
    åˆ›å»ºæœ¬åœ°æ¨¡å‹æ¥å£å®ä¾‹
    
    Args:
        model_type: æ¨¡å‹ç±»å‹
        **kwargs: é…ç½®å‚æ•°
        
    Returns:
        LocalModelInterfaceå®ä¾‹
    """
    return LocalModelInterface(model_type, **kwargs)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•Ollama
    print("=== æµ‹è¯•Ollama ===")
    ollama_interface = create_local_model_interface("ollama", model_name="qwen2.5:7b")
    ollama_interface.test_connection()
    
    # æµ‹è¯•LM Studio
    print("\n=== æµ‹è¯•LM Studio ===")
    lmstudio_interface = create_local_model_interface("lmstudio")
    lmstudio_interface.test_connection()
    
    # æµ‹è¯•vLLM
    print("\n=== æµ‹è¯•vLLM ===")
    vllm_interface = create_local_model_interface("vllm")
    vllm_interface.test_connection()
